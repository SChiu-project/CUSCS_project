{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sentiment Analysis using NLTK, AFINN, and BERT\n",
    "\n",
    "In this sentiment analysis project, we aim to analyze the sentiment of comments using different methods: NLTK, AFINN, and BERT. We will start by explaining each method and its role in the sentiment analysis pipeline."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. NLTK (Natural Language Toolkit)\n",
    "\n",
    "NLTK is a powerful Python library for natural language processing, including sentiment analysis. It provides various tools for text processing and analysis. In our project, we will use NLTK primarily for tokenization, removing stopwords, and calculating sentiment scores.\n",
    "\n",
    "Sentiment Scoring:\n",
    "NLTK offers various sentiment analysis approaches, including using pre-built sentiment lexicons. In this project, we will utilize the VADER (Valence Aware Dictionary and sentiment Reasoner) sentiment analyzer provided by NLTK. VADER is a lexicon and rule-based tool that assigns a sentiment score to each comment, indicating its positivity, negativity, or neutrality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "import nltk\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /Users/wailunchan/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Using the VADER sentiment analyzer\n",
    "\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "sid = SentimentIntensityAnalyzer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sentiment_score(token):\n",
    "    sentiment = sid.polarity_scores(token)\n",
    "    return sentiment['compound']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data which is created a \"cleaned_comments\" after nltk text preprocessing\n",
    "\n",
    "df = pd.read_csv(\"nltk_Reddit_comment_Jan-Jun23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment score for each word in the cleaned_comments and store in a single column\n",
    "\n",
    "df['sentiment_scores'] = df['cleaned_comments'].apply(lambda x: [get_sentiment_score(token) for token in x.split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Flatten the list of lists into a single list (handle non-list values)\n",
    "\n",
    "df['sentiment_scores'] = df['sentiment_scores'].apply(lambda x: [score for score in x if score != 0.0] if isinstance(x, list) else x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine sentiment scores of each word into a single list\n",
    "\n",
    "df['sentiment_scores'] = df['sentiment_scores'].apply(lambda x: ' '.join(str(score) for score in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the useless columns\n",
    "\n",
    "df.drop('Unnamed: 0.1', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>comments</th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Adj Close_x</th>\n",
       "      <th>Volume_x</th>\n",
       "      <th>percent_chnage_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>Adj Close_y</th>\n",
       "      <th>Volume_y</th>\n",
       "      <th>percent_chnage_y</th>\n",
       "      <th>Jump_y</th>\n",
       "      <th>Big_Jump_y</th>\n",
       "      <th>Drop_y</th>\n",
       "      <th>Big_Drop_y</th>\n",
       "      <th>cleaned_comments</th>\n",
       "      <th>sentiment_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3779.669922</td>\n",
       "      <td>3829.439941</td>\n",
       "      <td>3759.070068</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>4109390000</td>\n",
       "      <td>0.584975</td>\n",
       "      <td>...</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>4433060000</td>\n",
       "      <td>1.727757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings thread http wwwredditcomrwallstr...</td>\n",
       "      <td>-0.5423 -0.5423 -0.5423 0.4588 0.4215 0.25 0.5...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-14</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3763.989990</td>\n",
       "      <td>3796.409912</td>\n",
       "      <td>3721.560059</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>4199690000</td>\n",
       "      <td>0.701115</td>\n",
       "      <td>...</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>4481070000</td>\n",
       "      <td>0.896589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings thread http wwwredditcomrwallstr...</td>\n",
       "      <td>-0.3182 -0.3182 -0.4404 -0.4019 0.4404 0.4215 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>Cashed out up 56k today. Now cuddled up watchi...</td>\n",
       "      <td>3818.000000</td>\n",
       "      <td>3863.620117</td>\n",
       "      <td>3817.179932</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>4143800000</td>\n",
       "      <td>1.182816</td>\n",
       "      <td>...</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>4369060000</td>\n",
       "      <td>0.642036</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cashed k today cuddled watching netflix eating...</td>\n",
       "      <td>0.4215 -0.4215 -0.5859 -0.4215 -0.4939 -0.1531...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3939.810059</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>4041070000</td>\n",
       "      <td>1.967503</td>\n",
       "      <td>...</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>5302740000</td>\n",
       "      <td>1.720802</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings thread http wwwredditcomrwallstr...</td>\n",
       "      <td>0.5719 -0.3182 -0.6908 -0.5106 0.3612 0.3612 -...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>If 2008 was the Great Recession\\n\\nThen 2022 i...</td>\n",
       "      <td>3935.320068</td>\n",
       "      <td>3974.129883</td>\n",
       "      <td>3922.030029</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>4185300000</td>\n",
       "      <td>0.624596</td>\n",
       "      <td>...</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>5467080000</td>\n",
       "      <td>1.463067</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>great recession fake recession nflx gon na war...</td>\n",
       "      <td>0.6249 -0.4215 -0.4767 -0.4215 -0.1027 -0.3818...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        date                                           comments  \\\n",
       "0      0  2022-07-13  This week's [Earnings Thread](https://www.redd...   \n",
       "1      1  2022-07-14  This week's [Earnings Thread](https://www.redd...   \n",
       "2      2  2022-07-15  Cashed out up 56k today. Now cuddled up watchi...   \n",
       "3      3  2022-07-19  This week's [Earnings Thread](https://www.redd...   \n",
       "4      4  2022-07-20  If 2008 was the Great Recession\\n\\nThen 2022 i...   \n",
       "\n",
       "        Open_x       High_x        Low_x      Close_x  Adj Close_x  \\\n",
       "0  3779.669922  3829.439941  3759.070068  3801.780029  3801.780029   \n",
       "1  3763.989990  3796.409912  3721.560059  3790.379883  3790.379883   \n",
       "2  3818.000000  3863.620117  3817.179932  3863.159912  3863.159912   \n",
       "3  3860.729980  3939.810059  3860.729980  3936.689941  3936.689941   \n",
       "4  3935.320068  3974.129883  3922.030029  3959.899902  3959.899902   \n",
       "\n",
       "     Volume_x  percent_chnage_x  ...      Close_y  Adj Close_y    Volume_y  \\\n",
       "0  4109390000          0.584975  ...  11247.58008  11247.58008  4433060000   \n",
       "1  4199690000          0.701115  ...  11251.19043  11251.19043  4481070000   \n",
       "2  4143800000          1.182816  ...  11452.41992  11452.41992  4369060000   \n",
       "3  4041070000          1.967503  ...  11713.15039  11713.15039  5302740000   \n",
       "4  4185300000          0.624596  ...  11897.65039  11897.65039  5467080000   \n",
       "\n",
       "   percent_chnage_y  Jump_y  Big_Jump_y  Drop_y  Big_Drop_y  \\\n",
       "0          1.727757       0           1       0           0   \n",
       "1          0.896589       1           0       0           0   \n",
       "2          0.642036       1           0       0           0   \n",
       "3          1.720802       0           1       0           0   \n",
       "4          1.463067       0           1       0           0   \n",
       "\n",
       "                                    cleaned_comments  \\\n",
       "0  week earnings thread http wwwredditcomrwallstr...   \n",
       "1  week earnings thread http wwwredditcomrwallstr...   \n",
       "2  cashed k today cuddled watching netflix eating...   \n",
       "3  week earnings thread http wwwredditcomrwallstr...   \n",
       "4  great recession fake recession nflx gon na war...   \n",
       "\n",
       "                                    sentiment_scores  \n",
       "0  -0.5423 -0.5423 -0.5423 0.4588 0.4215 0.25 0.5...  \n",
       "1  -0.3182 -0.3182 -0.4404 -0.4019 0.4404 0.4215 ...  \n",
       "2  0.4215 -0.4215 -0.5859 -0.4215 -0.4939 -0.1531...  \n",
       "3  0.5719 -0.3182 -0.6908 -0.5106 0.3612 0.3612 -...  \n",
       "4  0.6249 -0.4215 -0.4767 -0.4215 -0.1027 -0.3818...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"nltk_sentiment_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. AFINN (Affective Norms for English Words)\n",
    "\n",
    "AFINN is a sentiment analysis tool that assigns pre-computed sentiment scores to individual words. Each word is given a score ranging from negative to positive, representing its sentiment intensity. We will use AFINN to calculate the overall sentiment score of each comment by summing up the sentiment scores of its constituent words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download 3 lexicon from https://github.com/fnielsen/afinn/blob/master/afinn/data/AFINN-111.txt\n",
    "\n",
    "AFINN-111.txt\n",
    "AFINN-96.txt\n",
    "AFINN-en-165.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the AFINN lexicon into a Python dictionary\n",
    "\n",
    "def load_sentiment_lexicon(filename):\n",
    "    sentiment_scores = {}\n",
    "    with open(filename, 'r') as file:\n",
    "        for line in file:\n",
    "            word, score = line.strip().split('\\t')\n",
    "            sentiment_scores[word] = int(score)\n",
    "    return sentiment_scores\n",
    "\n",
    "afinn_111_lexicon = load_sentiment_lexicon('AFINN-111.txt')\n",
    "afinn_96_lexicon = load_sentiment_lexicon('AFINN-96.txt')\n",
    "afinn_en_lexicon = load_sentiment_lexicon('AFINN-en-165.txt')\n",
    "\n",
    "combined_lexicon = {**afinn_111_lexicon, **afinn_96_lexicon, **afinn_en_lexicon}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform sentiment analysis using AFINN\n",
    "\n",
    "def get_sentiment_score(token):\n",
    "    if token in combined_lexicon:\n",
    "        score = combined_lexicon[token]\n",
    "        if score != 0.0:\n",
    "            return combined_lexicon[token]\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Loading the Data which is created a \"cleaned_comments\" after spacy text preprocessing\n",
    "\n",
    "df = pd.read_csv(\"spacy_Reddit_comment_Jan-Jun23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate sentiment scores for each word in the comment using the get_sentiment_score \n",
    "\n",
    "df['sentiment_scores'] = df['cleaned_comments'].apply(lambda x: [get_sentiment_score(token) for token in x.split() if get_sentiment_score(token) is not None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the list of scores into a single comma-separated string, make each sentiment score becomes an individual entry \n",
    "\n",
    "df['sentiment_scores'] = df['sentiment_scores'].apply(lambda x: ', '.join(str(score) for score in x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop the useless columns\n",
    "\n",
    "df.drop('Unnamed: 0.1', axis=1, inplace=True)\n",
    "df.drop('Unnamed: 0', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>comments</th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Adj Close_x</th>\n",
       "      <th>Volume_x</th>\n",
       "      <th>percent_chnage_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>Adj Close_y</th>\n",
       "      <th>Volume_y</th>\n",
       "      <th>percent_chnage_y</th>\n",
       "      <th>Jump_y</th>\n",
       "      <th>Big_Jump_y</th>\n",
       "      <th>Drop_y</th>\n",
       "      <th>Big_Drop_y</th>\n",
       "      <th>cleaned_comments</th>\n",
       "      <th>sentiment_scores</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3779.669922</td>\n",
       "      <td>3829.439941</td>\n",
       "      <td>3759.070068</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>4109390000</td>\n",
       "      <td>0.584975</td>\n",
       "      <td>...</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>4433060000</td>\n",
       "      <td>1.727757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings threadhttpswwwredditcom r wallst...</td>\n",
       "      <td>-3, -4, -3, 3, 1, 4, -2, 2, 4, 2, 1, -2, 2, 1,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-14</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3763.989990</td>\n",
       "      <td>3796.409912</td>\n",
       "      <td>3721.560059</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>4199690000</td>\n",
       "      <td>0.701115</td>\n",
       "      <td>...</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>4481070000</td>\n",
       "      <td>0.896589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings threadhttpswwwredditcom r wallst...</td>\n",
       "      <td>-3, -3, -3, 2, -1, 2, -2, -2, -3, -4, 3, 3, 3,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>Cashed out up 56k today. Now cuddled up watchi...</td>\n",
       "      <td>3818.000000</td>\n",
       "      <td>3863.620117</td>\n",
       "      <td>3817.179932</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>4143800000</td>\n",
       "      <td>1.182816</td>\n",
       "      <td>...</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>4369060000</td>\n",
       "      <td>0.642036</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cashed 56k today cuddle watch netflix eat chur...</td>\n",
       "      <td>3, -2, -1, -3, -2, -2, -1, -1, 1, -3, -2, 1, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3939.810059</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>4041070000</td>\n",
       "      <td>1.967503</td>\n",
       "      <td>...</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>5302740000</td>\n",
       "      <td>1.720802</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings threadhttpswwwredditcom r wallst...</td>\n",
       "      <td>-3, -2, -4, 2, 2, -4, -2, 2, 1, 3, -3, -2, -4,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>If 2008 was the Great Recession\\n\\nThen 2022 i...</td>\n",
       "      <td>3935.320068</td>\n",
       "      <td>3974.129883</td>\n",
       "      <td>3922.030029</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>4185300000</td>\n",
       "      <td>0.624596</td>\n",
       "      <td>...</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>5467080000</td>\n",
       "      <td>1.463067</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008 great recession 2022 fake recession nflx ...</td>\n",
       "      <td>3, -2, -3, -2, -2, -3, -1, -4, -4, -3, 1, 2, 2...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   index        date                                           comments  \\\n",
       "0      0  2022-07-13  This week's [Earnings Thread](https://www.redd...   \n",
       "1      1  2022-07-14  This week's [Earnings Thread](https://www.redd...   \n",
       "2      2  2022-07-15  Cashed out up 56k today. Now cuddled up watchi...   \n",
       "3      3  2022-07-19  This week's [Earnings Thread](https://www.redd...   \n",
       "4      4  2022-07-20  If 2008 was the Great Recession\\n\\nThen 2022 i...   \n",
       "\n",
       "        Open_x       High_x        Low_x      Close_x  Adj Close_x  \\\n",
       "0  3779.669922  3829.439941  3759.070068  3801.780029  3801.780029   \n",
       "1  3763.989990  3796.409912  3721.560059  3790.379883  3790.379883   \n",
       "2  3818.000000  3863.620117  3817.179932  3863.159912  3863.159912   \n",
       "3  3860.729980  3939.810059  3860.729980  3936.689941  3936.689941   \n",
       "4  3935.320068  3974.129883  3922.030029  3959.899902  3959.899902   \n",
       "\n",
       "     Volume_x  percent_chnage_x  ...      Close_y  Adj Close_y    Volume_y  \\\n",
       "0  4109390000          0.584975  ...  11247.58008  11247.58008  4433060000   \n",
       "1  4199690000          0.701115  ...  11251.19043  11251.19043  4481070000   \n",
       "2  4143800000          1.182816  ...  11452.41992  11452.41992  4369060000   \n",
       "3  4041070000          1.967503  ...  11713.15039  11713.15039  5302740000   \n",
       "4  4185300000          0.624596  ...  11897.65039  11897.65039  5467080000   \n",
       "\n",
       "   percent_chnage_y  Jump_y  Big_Jump_y  Drop_y  Big_Drop_y  \\\n",
       "0          1.727757       0           1       0           0   \n",
       "1          0.896589       1           0       0           0   \n",
       "2          0.642036       1           0       0           0   \n",
       "3          1.720802       0           1       0           0   \n",
       "4          1.463067       0           1       0           0   \n",
       "\n",
       "                                    cleaned_comments  \\\n",
       "0  week earnings threadhttpswwwredditcom r wallst...   \n",
       "1  week earnings threadhttpswwwredditcom r wallst...   \n",
       "2  cashed 56k today cuddle watch netflix eat chur...   \n",
       "3  week earnings threadhttpswwwredditcom r wallst...   \n",
       "4  2008 great recession 2022 fake recession nflx ...   \n",
       "\n",
       "                                    sentiment_scores  \n",
       "0  -3, -4, -3, 3, 1, 4, -2, 2, 4, 2, 1, -2, 2, 1,...  \n",
       "1  -3, -3, -3, 2, -1, 2, -2, -2, -3, -4, 3, 3, 3,...  \n",
       "2  3, -2, -1, -3, -2, -2, -1, -1, 1, -3, -2, 1, 3...  \n",
       "3  -3, -2, -4, 2, 2, -4, -2, 2, 1, 3, -3, -2, -4,...  \n",
       "4  3, -2, -3, -2, -2, -3, -1, -4, -4, -3, 1, 2, 2...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"afinn_sentiment_scores.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. BERT (Bidirectional Encoder Representations from Transformers)\n",
    "\n",
    "BERT is a state-of-the-art natural language processing model developed by Google. Unlike traditional methods like NLTK and AFINN, BERT is a deep learning model that can capture complex linguistic patterns and contextual information. In this project, we will use the BERT model, fine-tuned on a sentiment analysis task, to predict the sentiment of each comment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade transformers bert-for-tf2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Original Data\n",
    "\n",
    "df = pd.read_csv(\"consol_Reddit_comment_Jan-Jun23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n",
      "\n",
      "systemMemory: 16.00 GB\n",
      "maxCacheSize: 5.33 GB\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-24 14:37:18.830164: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2023-07-24 14:37:18.830312: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "All model checkpoint layers were used when initializing TFBertForSequenceClassification.\n",
      "\n",
      "All the layers of TFBertForSequenceClassification were initialized from the model checkpoint at nlptown/bert-base-multilingual-uncased-sentiment.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertForSequenceClassification for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the Model and Tokenizer\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')\n",
    "model = TFBertForSequenceClassification.from_pretrained('nlptown/bert-base-multilingual-uncased-sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Sentiment Analysis Function\n",
    "\n",
    "def sentiment_score(review):\n",
    "    tokens = tokenizer.encode(review, return_tensors='tf', max_length=512, truncation=True)\n",
    "    result = model(tokens)\n",
    "    return int(tf.argmax(result.logits, axis=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split comments into sentences and apply 'sentiment_score' function\n",
    "\n",
    "df['sentiment_scores'] = df['comments'].str.split('.').apply(lambda sentences: [sentiment_score(sentence) for sentence in sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame with sentiment scores to a new CSV file\n",
    "\n",
    "df.to_csv('bert_sentiment_analysis.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
