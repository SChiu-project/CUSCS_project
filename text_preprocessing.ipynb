{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Comments Data using spaCy, re, and NLTK\n",
    "\n",
    "Introduction\n",
    "In this Jupyter Notebook, we will demonstrate how to clean comments data using the spaCy library for tokenization, the re (regular expression) library for removing special characters, and NLTK for stop word removal and lemmatization. The goal is to preprocess the raw comments data and prepare it for further analysis or natural language processing tasks.\n",
    "\n",
    "Important Notes: The spaCy and NLTK are two separate library, it is apply individually in this notebook. Please don't run all the code in one time from this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 1st Part: SpaCY & re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install spaCY\n",
    "\n",
    "pip install spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download Spacy language model\n",
    "\n",
    "python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "\n",
    "df = pd.read_csv(\"consol_Reddit_comment_Jan-Jun23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text Preprocessing with spaCy and re\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.max_length = 3000000  # Set a higher value based on your needs\n",
    "spacy_stopwords = nlp.Defaults.stop_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'any', 'until', 'one', 'thereafter', 'everything', 'around', 'not', 'perhaps', 'but', 'alone', 'nobody', 'several', 'that', 'whoever', 'it', 'me', 'none', 'themselves', 'still', '’ve', 'empty', 'only', 'get', 'is', 'see', '‘ve', 'full', 'may', 'mine', 'too', 'nine', 'anyhow', 'thence', 'everyone', 'meanwhile', 'next', 'above', 'per', 'which', 'wherever', 'used', 'amount', 'last', 'made', 'there', 'formerly', '’s', 'together', 'yet', 'indeed', 'behind', 'them', 'an', 'back', 'eleven', 'for', 'regarding', 'other', 'hereupon', 'they', 'thereupon', 'becoming', 'twelve', 'give', 'so', 'the', 'more', '‘re', 'ourselves', 'below', 'latter', 'beforehand', 'you', \"'m\", 'once', 'upon', 'now', 'others', 'are', 'he', 'might', 'should', 'third', 'with', 'nowhere', 'such', 'neither', 'whence', 'was', '‘ll', 'make', 'no', 'part', 'who', 'sometime', 'because', 'elsewhere', 'except', 'itself', 'us', 'throughout', 'then', 'less', 'whole', 'up', 'hereby', 'move', 'did', 'himself', 'though', 'unless', 'ca', 'hers', 'front', 'being', 'keep', 'bottom', 'fifty', 'among', 'yourselves', 'across', 'many', 'into', 'her', 'whenever', 'on', 'besides', \"'d\", 'very', 'else', 'first', 'here', 'have', 'over', 'some', 'how', 'however', 'although', 'also', 'thru', 'if', 'again', 'your', 'during', 'doing', 'put', 'serious', 'does', 'hereafter', 'noone', '‘d', 'six', 'anyone', 'amongst', 'everywhere', 'yourself', 'whereupon', 'within', 'therein', 'became', 'afterwards', 'ever', 'various', 'herein', 'three', \"'ll\", 'about', 'hundred', 'someone', 'beside', 'take', 'please', 'otherwise', 'whereby', 'toward', 'five', 'therefore', 'somehow', '’ll', 'whose', 'seeming', '‘s', 'whereas', 'has', 'n’t', 'becomes', 'between', 'since', '’d', 'using', 'myself', 'thereby', 'their', 'mostly', 'either', 'show', 'twenty', 'seemed', \"'ve\", 'nevertheless', 'whom', 'by', 'these', 'become', 'really', 'go', 'yours', 'along', '’re', 'and', 'each', 'where', 'two', 'enough', 'be', 'we', 'against', 'eight', 'when', 'will', 'another', 'well', 'seems', 'without', 'namely', 'its', 'always', 'somewhere', 'onto', 'beyond', 'out', 'just', 'after', 'as', 'herself', 'least', 'thus', 'even', 'nothing', 'this', 'wherein', 'why', 'down', 'former', 'few', 'i', 'much', 'whither', 'moreover', 'his', 'something', 'ours', 'must', 'what', 'almost', 'off', 'often', 'ten', 'same', 'than', 'a', 'all', 'done', 'she', '’m', 'from', 'anything', 'whatever', 'whether', 'name', 'under', 'via', 'my', 'quite', 'could', 'further', 'sixty', 'both', 'seem', '‘m', 'side', 'forty', 'to', 'while', \"'re\", 'four', 'rather', 'anywhere', 'fifteen', 'hence', 'whereafter', 'latterly', \"'s\", 'had', 'were', 'say', \"n't\", 'top', 'at', 'cannot', 'through', 'before', 'been', 'every', 'can', 'already', 'our', 'or', 'call', 'never', 'own', 'of', 'most', 'nor', 'towards', 'do', 'sometimes', 'in', 're', 'am', 'n‘t', 'him', 'would', 'anyway', 'those', 'due'}\n",
      "326\n"
     ]
    }
   ],
   "source": [
    "# Print default spacy stopwords\n",
    "\n",
    "print(spacy_stopwords)\n",
    "print(len(spacy_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set stopwords by yourself\n",
    "#custom_stopwords = ['reddit', 'stock', 'market', 'company', 'invest', 'close', 'green', 'candle', 'like']\n",
    "\n",
    "custom_stopwords = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization with spaCy\n",
    "\n",
    "def clean_comment(comment, custom_stopwords=None):\n",
    "    doc = nlp(comment)\n",
    "    cleaned_tokens = []\n",
    "    \n",
    "    if custom_stopwords is None:\n",
    "        custom_stopwords = []\n",
    "    \n",
    "    for token in doc:\n",
    "        if not token.is_stop and not token.is_punct and token.lemma_.lower() not in custom_stopwords:\n",
    "            cleaned_tokens.append(token.lemma_.lower())\n",
    "    \n",
    "    cleaned_comment = \" \".join(cleaned_tokens)\n",
    "    \n",
    "    # Remove extra whitespace and meaningless symbols\n",
    "    cleaned_comment = re.sub(r'\\s+', ' ', cleaned_comment)\n",
    "    cleaned_comment = re.sub(r'[^a-zA-Z0-9\\s]', '', cleaned_comment)\n",
    "    \n",
    "    return cleaned_comment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply to the data\n",
    "\n",
    "df['cleaned_comments'] = df['comments'].apply(lambda x: clean_comment(x, custom_stopwords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>comments</th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Adj Close_x</th>\n",
       "      <th>Volume_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Low_y</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>Adj Close_y</th>\n",
       "      <th>Volume_y</th>\n",
       "      <th>percent_chnage_y</th>\n",
       "      <th>Jump_y</th>\n",
       "      <th>Big_Jump_y</th>\n",
       "      <th>Drop_y</th>\n",
       "      <th>Big_Drop_y</th>\n",
       "      <th>cleaned_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3779.669922</td>\n",
       "      <td>3829.439941</td>\n",
       "      <td>3759.070068</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>4109390000</td>\n",
       "      <td>...</td>\n",
       "      <td>11031.26953</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>4433060000</td>\n",
       "      <td>1.727757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings threadhttpswwwredditcom r wallst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-14</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3763.989990</td>\n",
       "      <td>3796.409912</td>\n",
       "      <td>3721.560059</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>4199690000</td>\n",
       "      <td>...</td>\n",
       "      <td>11005.92969</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>4481070000</td>\n",
       "      <td>0.896589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings threadhttpswwwredditcom r wallst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>Cashed out up 56k today. Now cuddled up watchi...</td>\n",
       "      <td>3818.000000</td>\n",
       "      <td>3863.620117</td>\n",
       "      <td>3817.179932</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>4143800000</td>\n",
       "      <td>...</td>\n",
       "      <td>11295.33008</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>4369060000</td>\n",
       "      <td>0.642036</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cashed 56k today cuddle watch netflix eat chur...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3939.810059</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>4041070000</td>\n",
       "      <td>...</td>\n",
       "      <td>11448.96973</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>5302740000</td>\n",
       "      <td>1.720802</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings threadhttpswwwredditcom r wallst...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>If 2008 was the Great Recession\\n\\nThen 2022 i...</td>\n",
       "      <td>3935.320068</td>\n",
       "      <td>3974.129883</td>\n",
       "      <td>3922.030029</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>4185300000</td>\n",
       "      <td>...</td>\n",
       "      <td>11703.36035</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>5467080000</td>\n",
       "      <td>1.463067</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2008 great recession 2022 fake recession nflx ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index        date  \\\n",
       "0           0      0  2022-07-13   \n",
       "1           1      1  2022-07-14   \n",
       "2           2      2  2022-07-15   \n",
       "3           3      3  2022-07-19   \n",
       "4           4      4  2022-07-20   \n",
       "\n",
       "                                            comments       Open_x  \\\n",
       "0  This week's [Earnings Thread](https://www.redd...  3779.669922   \n",
       "1  This week's [Earnings Thread](https://www.redd...  3763.989990   \n",
       "2  Cashed out up 56k today. Now cuddled up watchi...  3818.000000   \n",
       "3  This week's [Earnings Thread](https://www.redd...  3860.729980   \n",
       "4  If 2008 was the Great Recession\\n\\nThen 2022 i...  3935.320068   \n",
       "\n",
       "        High_x        Low_x      Close_x  Adj Close_x    Volume_x  ...  \\\n",
       "0  3829.439941  3759.070068  3801.780029  3801.780029  4109390000  ...   \n",
       "1  3796.409912  3721.560059  3790.379883  3790.379883  4199690000  ...   \n",
       "2  3863.620117  3817.179932  3863.159912  3863.159912  4143800000  ...   \n",
       "3  3939.810059  3860.729980  3936.689941  3936.689941  4041070000  ...   \n",
       "4  3974.129883  3922.030029  3959.899902  3959.899902  4185300000  ...   \n",
       "\n",
       "         Low_y      Close_y  Adj Close_y    Volume_y  percent_chnage_y  \\\n",
       "0  11031.26953  11247.58008  11247.58008  4433060000          1.727757   \n",
       "1  11005.92969  11251.19043  11251.19043  4481070000          0.896589   \n",
       "2  11295.33008  11452.41992  11452.41992  4369060000          0.642036   \n",
       "3  11448.96973  11713.15039  11713.15039  5302740000          1.720802   \n",
       "4  11703.36035  11897.65039  11897.65039  5467080000          1.463067   \n",
       "\n",
       "   Jump_y  Big_Jump_y  Drop_y  Big_Drop_y  \\\n",
       "0       0           1       0           0   \n",
       "1       1           0       0           0   \n",
       "2       1           0       0           0   \n",
       "3       0           1       0           0   \n",
       "4       0           1       0           0   \n",
       "\n",
       "                                    cleaned_comments  \n",
       "0  week earnings threadhttpswwwredditcom r wallst...  \n",
       "1  week earnings threadhttpswwwredditcom r wallst...  \n",
       "2  cashed 56k today cuddle watch netflix eat chur...  \n",
       "3  week earnings threadhttpswwwredditcom r wallst...  \n",
       "4  2008 great recession 2022 fake recession nflx ...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"spacy_Reddit_comment_Jan-Jun23.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The 2nd Part: NLTK and re\n",
    "\n",
    "Important Notes: The NLTK and spaCy are two separate library, it is apply individually in this notebook. Please don't run all the code from this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install NLTK (Natural Language Toolkit)\n",
    "\n",
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing Required Libraries\n",
    "\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the Data\n",
    "\n",
    "df = pd.read_csv(\"consol_Reddit_comment_Jan-Jun23.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/wailunchan/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/wailunchan/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/wailunchan/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK resources\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define my own project-specific stop words\n",
    "# Add any additional project-specific stopwords\n",
    "\n",
    "custom_stopwords = ['', '']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the NLTK lemmatizer\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to clean and preprocess the words\n",
    "\n",
    "def clean_words(text):\n",
    "    # Tokenize the text into individual words\n",
    "    tokens = word_tokenize(text.lower())\n",
    "\n",
    "    # Remove special characters and numbers using regex\n",
    "    tokens = [re.sub(r'[^a-zA-Z\\s]', '', token) for token in tokens]\n",
    "\n",
    "    # Remove white spaces from the tokens\n",
    "    tokens = [token.strip() for token in tokens]\n",
    "\n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english') + custom_stopwords)\n",
    "    tokens = [token for token in tokens if token not in stop_words]\n",
    "\n",
    "    # Lemmatize the words\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(token) for token in tokens]\n",
    "\n",
    "    # Filter out empty strings ('')\n",
    "    lemmatized_tokens = [token for token in lemmatized_tokens if token]\n",
    "\n",
    "    # Join the lemmatized tokens back into a single string\n",
    "    cleaned_text = ' '.join(lemmatized_tokens)\n",
    "\n",
    "    return cleaned_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n",
      "179\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))\n",
    "print(len(stopwords.words('english')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply the comments cleaning function to your comments data\n",
    "\n",
    "df['cleaned_comments'] = df['comments'].apply(clean_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>index</th>\n",
       "      <th>date</th>\n",
       "      <th>comments</th>\n",
       "      <th>Open_x</th>\n",
       "      <th>High_x</th>\n",
       "      <th>Low_x</th>\n",
       "      <th>Close_x</th>\n",
       "      <th>Adj Close_x</th>\n",
       "      <th>Volume_x</th>\n",
       "      <th>...</th>\n",
       "      <th>Low_y</th>\n",
       "      <th>Close_y</th>\n",
       "      <th>Adj Close_y</th>\n",
       "      <th>Volume_y</th>\n",
       "      <th>percent_chnage_y</th>\n",
       "      <th>Jump_y</th>\n",
       "      <th>Big_Jump_y</th>\n",
       "      <th>Drop_y</th>\n",
       "      <th>Big_Drop_y</th>\n",
       "      <th>cleaned_comments</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2022-07-13</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3779.669922</td>\n",
       "      <td>3829.439941</td>\n",
       "      <td>3759.070068</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>3801.780029</td>\n",
       "      <td>4109390000</td>\n",
       "      <td>...</td>\n",
       "      <td>11031.26953</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>11247.58008</td>\n",
       "      <td>4433060000</td>\n",
       "      <td>1.727757</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings thread http wwwredditcomrwallstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2022-07-14</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3763.989990</td>\n",
       "      <td>3796.409912</td>\n",
       "      <td>3721.560059</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>3790.379883</td>\n",
       "      <td>4199690000</td>\n",
       "      <td>...</td>\n",
       "      <td>11005.92969</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>11251.19043</td>\n",
       "      <td>4481070000</td>\n",
       "      <td>0.896589</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings thread http wwwredditcomrwallstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>2022-07-15</td>\n",
       "      <td>Cashed out up 56k today. Now cuddled up watchi...</td>\n",
       "      <td>3818.000000</td>\n",
       "      <td>3863.620117</td>\n",
       "      <td>3817.179932</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>3863.159912</td>\n",
       "      <td>4143800000</td>\n",
       "      <td>...</td>\n",
       "      <td>11295.33008</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>11452.41992</td>\n",
       "      <td>4369060000</td>\n",
       "      <td>0.642036</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>cashed k today cuddled watching netflix eating...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>2022-07-19</td>\n",
       "      <td>This week's [Earnings Thread](https://www.redd...</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3939.810059</td>\n",
       "      <td>3860.729980</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>3936.689941</td>\n",
       "      <td>4041070000</td>\n",
       "      <td>...</td>\n",
       "      <td>11448.96973</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>11713.15039</td>\n",
       "      <td>5302740000</td>\n",
       "      <td>1.720802</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>week earnings thread http wwwredditcomrwallstr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>2022-07-20</td>\n",
       "      <td>If 2008 was the Great Recession\\n\\nThen 2022 i...</td>\n",
       "      <td>3935.320068</td>\n",
       "      <td>3974.129883</td>\n",
       "      <td>3922.030029</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>3959.899902</td>\n",
       "      <td>4185300000</td>\n",
       "      <td>...</td>\n",
       "      <td>11703.36035</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>11897.65039</td>\n",
       "      <td>5467080000</td>\n",
       "      <td>1.463067</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>great recession fake recession nflx gon na war...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  index        date  \\\n",
       "0           0      0  2022-07-13   \n",
       "1           1      1  2022-07-14   \n",
       "2           2      2  2022-07-15   \n",
       "3           3      3  2022-07-19   \n",
       "4           4      4  2022-07-20   \n",
       "\n",
       "                                            comments       Open_x  \\\n",
       "0  This week's [Earnings Thread](https://www.redd...  3779.669922   \n",
       "1  This week's [Earnings Thread](https://www.redd...  3763.989990   \n",
       "2  Cashed out up 56k today. Now cuddled up watchi...  3818.000000   \n",
       "3  This week's [Earnings Thread](https://www.redd...  3860.729980   \n",
       "4  If 2008 was the Great Recession\\n\\nThen 2022 i...  3935.320068   \n",
       "\n",
       "        High_x        Low_x      Close_x  Adj Close_x    Volume_x  ...  \\\n",
       "0  3829.439941  3759.070068  3801.780029  3801.780029  4109390000  ...   \n",
       "1  3796.409912  3721.560059  3790.379883  3790.379883  4199690000  ...   \n",
       "2  3863.620117  3817.179932  3863.159912  3863.159912  4143800000  ...   \n",
       "3  3939.810059  3860.729980  3936.689941  3936.689941  4041070000  ...   \n",
       "4  3974.129883  3922.030029  3959.899902  3959.899902  4185300000  ...   \n",
       "\n",
       "         Low_y      Close_y  Adj Close_y    Volume_y  percent_chnage_y  \\\n",
       "0  11031.26953  11247.58008  11247.58008  4433060000          1.727757   \n",
       "1  11005.92969  11251.19043  11251.19043  4481070000          0.896589   \n",
       "2  11295.33008  11452.41992  11452.41992  4369060000          0.642036   \n",
       "3  11448.96973  11713.15039  11713.15039  5302740000          1.720802   \n",
       "4  11703.36035  11897.65039  11897.65039  5467080000          1.463067   \n",
       "\n",
       "   Jump_y  Big_Jump_y  Drop_y  Big_Drop_y  \\\n",
       "0       0           1       0           0   \n",
       "1       1           0       0           0   \n",
       "2       1           0       0           0   \n",
       "3       0           1       0           0   \n",
       "4       0           1       0           0   \n",
       "\n",
       "                                    cleaned_comments  \n",
       "0  week earnings thread http wwwredditcomrwallstr...  \n",
       "1  week earnings thread http wwwredditcomrwallstr...  \n",
       "2  cashed k today cuddled watching netflix eating...  \n",
       "3  week earnings thread http wwwredditcomrwallstr...  \n",
       "4  great recession fake recession nflx gon na war...  \n",
       "\n",
       "[5 rows x 27 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"nltk_Reddit_comment_Jan-Jun23.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
